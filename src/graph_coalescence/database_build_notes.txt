Locate the jsonl files that you want to consume.

edit the build_redis_files.py file and insure that the path for the input file and output files are legit. then run:
python build_redis_files.py

Standing up a redis server is easily done using the helm charts to create a new version of the AC deployment. once the new deployment
is up port-forward the redis DB to the local host.

Then in another session on the same host run the redis loader:

python load_redis.py

when this loading is complete dont forget to do a "redis-cli save" to insure the data is persisted to disk.

When redis has persisted everything to disk on the new version will be ready. at that point "helm uninstall" the current running version and upgrade the new
version after corrected the web server URL in the values file.

################### STOP ##############
These are the old instructions, but they've been replaced with a simpler approach, starting with the KGX files that
get loaded into robokop.
To build the redis db, log into robokopdev.renci.org

Find where the neo4j you want is running with: kubectl -n translator-robokop get pods -o wide

Get the bolt (7687) port with: kubectl -n translator-robokop get svc

With those two pieces of information, you can launch cypher shell (you know the password):

cypher-shell -a stars-k6.edc.renci.org:31333 -u neo4j -p XXXXXXXXX

NOTE, you should probably do this in a screen, it takes a long time to run.

Insure APOC is installed (it should be by default) and then you can run in the shell:

WITH "MATCH path = (a)-[x]->(b)
      RETURN a.id AS source_id, labels(a) AS source_labels, type(x) AS predicate,
             b.id AS target_id, labels(b) AS target_labels" AS query
      CALL apoc.export.csv.query(query, "everything.csv", {})
      YIELD file, source, format, nodes, relationships, properties, time, rows, batchSize, batches, done, data
      RETURN file, source, format, nodes, relationships, properties, time, rows, batchSize, batches, done, data;

The path there is local to the database.  you can find everything.csv in /projects/stars/Data_services/AC.

Building the redis is a 2 step process.  It required a bit of memory, so I have been using a hatteras node like
sinteractive -m 32000


